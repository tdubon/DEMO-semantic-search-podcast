{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Weaviate embedded for question/answering on your vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we use Weaviate Embedded to create a vector store and question/answering from transcribed podcasts. The steps will include uploading your data from a local store, and creating a schema as well as an object store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from weaviate.util import generate_uuid5\n",
    "import json\n",
    "import helper\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedded Weaviate instance allows us to have the source data saved and retrieved locally, while having access to the vectorizing modules available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions(\n",
    "        additional_env_vars={\n",
    "        \"ENABLE_MODULES\": \"text2vec-openai\",\n",
<<<<<<< HEAD
    "        \"OPENAI_APIKEY\": \"your-key\"  # Replace with your OpenAI key\n",
=======
    "        \"OPENAI_APIKEY\": \"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\"  # Replace with your OpenAI key\n",
>>>>>>> d1147d6 (Edited notebook to include an example connecting to the server with docker)
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the client information to confirm the modules are loaded.\n",
    "meta_info = client.get_meta()\n",
    "print(json.dumps(meta_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we setup the schema, an outline requiring the data type, vectorizer and the list of classes. Note that it is essential to have your data cleaned and the categories clearly identified for this step. If using your own vectorizer, \"none\" should be specified for \"vectorizer\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.schema.delete_all()\n",
    "schema = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"class\": \"Podcast\",\n",
    "            \"vectorizer\": \"text2vec-openai\",\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"name\": \"title\",\n",
    "                    \"dataType\": [\"text\"]\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"transcript\",\n",
    "                    \"dataType\": [\"text\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we load the locally stored data (in json format) and create a function definition for an add_podcast object. \n",
    "\n",
    "The name of the object represents the highest level classification for your data, indicated below as podcast_object (in dictionary type). Target class represents the next level in the classification of your data. Here we indicate it below as the string \"Podcast\", but note that multiple classes could have been specified, for example, if we had different categories of podcasts, such as English, Spanish, etc.\n",
    "\n",
    "The function definition below is implementing batch_size=1. Note that with larger amounts of data you will want to adjust this setting. Per the documentation: \"batch imports are used to maximize import speed and minimize network latency. Batch import processes multiple objects per request, and clients can parallelize the process.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/tdubon/DEMO-semantic-search-podcast/data/podcast_ds.json\", 'r') as f:\n",
    "    datastore = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(datastore, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we define the batch and the uuid.\n",
    "\n",
    "Batch definition is helpful because it's \"a way of importing/creating objects and references in bulk using a single API request to the Weaviate server.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_podcasts(batch_size = 1):\n",
    "    client.batch.configure(batch_size=1)\n",
    "    with client.batch as batch:\n",
    "        for i, d in enumerate(datastore):\n",
    "            print(f\"importing podcast: {i+1}\")\n",
    "            properties = {\n",
    "                \"title\": d[\"title\"],\n",
    "                \"transcript\": d[\"transcript\"]\n",
    "            }\n",
    "            podcast_uuid = generate_uuid5('podcast', d[\"title\"] + d[\"transcript\"])\n",
    "        \n",
    "            batch.add_data_object(\n",
    "                data_object=properties, \n",
    "                class_name= \"Podcast\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_podcasts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you implement the pipeline and query your data, such as semantic search, generative search, question/answering. In this example we use nearText with the module text2vec-openai which implments text-embedding-ada-002. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question answering - search \n",
    "\n",
    "response = (\n",
    "    client.query\n",
    "    .get(\"Podcast\", [\"transcript\"])\n",
    "    .with_near_text({\"concepts\": [\"semantic search\"]})\n",
    "    .with_limit(3)\n",
    "    .with_additional([\"distance\"])\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SemanticSearch",
   "language": "python",
   "name": "semanticsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
