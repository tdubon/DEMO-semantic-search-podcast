{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Weaviate embedded for question/answering on your vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we use Weaviate Embedded to create a vector store and question/answering from transcribed podcasts. The steps will include uploading your data from a local store, and creating a schema as well as an object store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from weaviate.util import generate_uuid5\n",
    "import json\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedded Weaviate instance allows us to have the source data saved and retrieved locally, while having access to the vectorizing modules available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions(\n",
    "        additional_env_vars={\n",
    "        \"ENABLE_MODULES\":\n",
    "        \"text2vec-openai,text2vec-huggingface\"}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the client information to confirm the modules are loaded.\n",
    "meta_info = client.get_meta()\n",
    "print(json.dumps(meta_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we setup the schema, an outline requiring the data type, vectorizer and the list of classes. Note that it is essential to have your data cleaned and the categories clearly identified for this step. If using your own vectorizer, \"none\" should be specified for \"vectorizer\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.schema.delete_all()\n",
    "schema = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"class\": \"Podcast\",\n",
    "            \"vectorizer\": \"text2vec-cohere\",\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"name\": \"title\",\n",
    "                    \"dataType\": [\"text\"]\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"transcript\",\n",
    "                    \"dataType\": [\"text\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we load the locally stored data (in json format) and create a function definition for an add_podcast object. \n",
    "\n",
    "The name of the object represents the highest level classification for your data, indicated below as podcast_object (in dictionary type). Target class represents the next level in the classification of your data. Here we indicate it below as the string \"Podcast\", but note that multiple classes could have been specified, for example, if we had different categories of podcasts, such as English, Spanish, etc.\n",
    "\n",
    "The function definition below is implementing batch_size=1. Note that with larger amounts of data you will want to adjust this setting. Per the documentation: \"batch imports are used to maximize import speed and minimize network latency. Batch import processes multiple objects per request, and clients can parallelize the process.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/tdubon/DEMO-semantic-search-podcast/data/podcast_ds.json\", 'r') as f:\n",
    "    datastore = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_podcasts(batch_size = 1):\n",
    "    no_items_in_batch = 0\n",
    "    for item in datastore:\n",
    "        podcast_object = {\n",
    "            \"title\": item[\"title\"],\n",
    "            \"transcript\": item[\"transcript\"]\n",
    "        }\n",
    "\n",
    "        podcast_uuid = generate_uuid5('podcast', item[\"title\"] + item[\"transcript\"])\n",
    "        client.batch.add_data_object(podcast_object, \"Podcast\", podcast_uuid)\n",
    "        no_items_in_batch += 1\n",
    "\n",
    "        if no_items_in_batch >= batch_size:\n",
    "            results = client.batch.create_objects()\n",
    "\n",
    "            for result in results:\n",
    "                    if result['result'] != {}:\n",
    "                        helper.log(result['result'])\n",
    "\n",
    "            message = str(item[\"title\"]) + ' imported'\n",
    "            helper.log(message)\n",
    "\n",
    "            no_items_in_batch = 0\n",
    "\n",
    "    client.batch.create_objects()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_podcasts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you would implement the pipeline for whatever steps you need to take to query your data, such as semantic search, generative search, question/answering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"errors\": [\n",
      "    {\n",
      "      \"locations\": [\n",
      "        {\n",
      "          \"column\": 23,\n",
      "          \"line\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"message\": \"Unknown argument \\\"ask\\\" on field \\\"Podcast\\\" of type \\\"GetObjectsObj\\\".\",\n",
      "      \"path\": null\n",
      "    },\n",
      "    {\n",
      "      \"locations\": [\n",
      "        {\n",
      "          \"column\": 99,\n",
      "          \"line\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"message\": \"Cannot query field \\\"question\\\" on type \\\"Podcast\\\".\",\n",
      "      \"path\": null\n",
      "    },\n",
      "    {\n",
      "      \"locations\": [\n",
      "        {\n",
      "          \"column\": 121,\n",
      "          \"line\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"message\": \"Cannot query field \\\"answer\\\" on type \\\"PodcastAdditional\\\".\",\n",
      "      \"path\": null\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Question answering - search \n",
    "\n",
    "response = (\n",
    "    client.query\n",
    "    .get(\"Podcast\", [\"transcript\"])\n",
    "    .with_near_text({\"concepts\": [\"biology\"]})\n",
    "    .with_limit(3)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(json.dumps(res, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
